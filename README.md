# Code repository for paper:

## Fusion of Learned Representations for Multimodal Sensor Data Classification


## Abstract
Time-Series data collected using body-worn sensors can be used to recognize activities of interest in various medical applications such as sleep studies. Recent advances in other domains, such as image recognition and natural language processing have shown that unlabeled data can still be useful when self-supervised techniques such as contrastive learning are used to generate meaningful feature space representations. Labeling data for Human Activity Recognition (HAR) and sleep disorder diagnosis (polysomnography) is difficult and requires trained professionals. In this work, we apply learned feature representation techniques to multimodal time-series data. By using signal-specific representations, based on self-supervised and supervised learning, the channels can be evaluated to determine if they are likely to contribute to correct classification. The learned representation embeddings are then used to process each channel into a new feature space that serves as input into a neural network. This results in a better understanding of the importance of each signal modality as well as the potential applicability of newer self-supervised techniques to time-series data.

## Cite as:

[https://link.springer.com/chapter/10.1007/978-3-031-34111-3_34](https://link.springer.com/chapter/10.1007/978-3-031-34111-3_34)

```bibtex
@inproceedings{hinkle2023fusion,
  title={Fusion of Learned Representations for Multimodal Sensor Data Classification},
  author={Hinkle, Lee B and Atkinson, Gentry and Metsis, Vangelis},
  booktitle={IFIP International Conference on Artificial Intelligence Applications and Innovations},
  pages={404--415},
  year={2023},
  organization={Springer}
}
```
